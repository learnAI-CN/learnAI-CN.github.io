<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" content="我对DNN知识系统整理的一次尝试">
        
        <link rel="canonical" href="https://learnai-cn.github.io/01%20Getting%20started/06%20Backpropagation/">
        <link rel="shortcut icon" href="../../img/favicon.ico">

	<title>反向传播算法 - LearnAI-CN</title>

        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome-4.0.3.css" rel="stylesheet">
        <link rel="stylesheet" href="../../css/highlight.css">
        <link href="../../css/base.css" rel="stylesheet">
        <link href="../../css/style.css" rel="stylesheet">
        <link href="../../css/admonition_fix.css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
          ga('create', 'UA-80323542-1', 'auto');
          ga('send', 'pageview');
        </script>
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <!-- Main title -->
            <a class="navbar-brand" href="../..">LearnAI-CN</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            <!-- Main navigation -->
            <ul class="nav navbar-nav">
            
            
                <li >
                    <a href="../..">主页</a>
                </li>
            
            
            
                <li class="dropdown active">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">目录 <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        
<li >
    <a href="../../intro/">简介</a>
</li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">入门</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../01%20Introduction/">深度学习介绍</a>
</li>

        
            
<li >
    <a href="../02%20Perceptron/">感知机</a>
</li>

        
            
<li >
    <a href="../03%20XOR/">异或操作</a>
</li>

        
            
<li >
    <a href="../04%20S%20neural/">s型神经元</a>
</li>

        
            
<li >
    <a href="../05%20loss%20functional/">损失函数</a>
</li>

        
            
<li class="active">
    <a href="./">反向传播算法</a>
</li>

        
            
<li >
    <a href="../07%20Selflearning%20XOR/">自学习异或操作</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">走向深度</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../02%20HardWayToDeep/01%201Channel%20minist/">minist实现</a>
</li>

        
            
<li >
    <a href="../../02%20HardWayToDeep/02%20Gradient%20vanishing/">梯度消失/爆炸</a>
</li>

        
            
<li >
    <a href="../../02%20HardWayToDeep/03%20generalization/">泛化性</a>
</li>

        
            
<li >
    <a href="../../02%20HardWayToDeep/04%20overfitting/">过拟合/欠拟合</a>
</li>

        
            
<li >
    <a href="../../02%20HardWayToDeep/05%20regulization/">正则化</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">实现minist</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../02%20minist/01%20Convolution/">卷积层</a>
</li>

        
            
<li >
    <a href="../../02%20minist/02%20Pooling/">池化层</a>
</li>

        
            
<li >
    <a href="../../02%20minist/03%20Activation/">激活层</a>
</li>

        
            
<li >
    <a href="../../02%20minist/04%20Fully%20conection/">全连接层</a>
</li>

        
            
<li >
    <a href="../../02%20minist/05%20Optimization%20Algorithm/">优化函数</a>
</li>

        
            
<li >
    <a href="../../02%20minist/06%20Normalization/">归一化层</a>
</li>

        
            
<li >
    <a href="../../02%20minist/07%20LeNet/">再次实现minist</a>
</li>

        
            
<li >
    <a href="../../02%20minist/08%20WhyDCNNUseful/">为什么卷积有效</a>
</li>

        
            
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">深入介绍</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../02%20minist/09%20Deep%20introduce/01%20DNN%20feature/">DNN特性</a>
</li>

        
            
<li >
    <a href="../../02%20minist/09%20Deep%20introduce/02%20pooling%20BP/">池化层反向传播</a>
</li>

        
            
<li >
    <a href="../../02%20minist/09%20Deep%20introduce/03%20Receptive%20Field/">感受野</a>
</li>

        
    </ul>
  </li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">经典神经网络</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../03%20classicial%20model/01%20AlexNet/">AlexNet</a>
</li>

        
            
<li >
    <a href="../../03%20classicial%20model/02%20VGG/">VGG</a>
</li>

        
            
<li >
    <a href="../../03%20classicial%20model/03%20GoogleNet/">GoogLeNet</a>
</li>

        
            
<li >
    <a href="../../03%20classicial%20model/04%20ResNet/">ResNet</a>
</li>

        
            
<li >
    <a href="../../03%20classicial%20model/05%20DenseNet/">DenseNet</a>
</li>

        
            
<li >
    <a href="../../03%20classicial%20model/06%20SENet/">SENet</a>
</li>

        
            
<li >
    <a href="../../03%20classicial%20model/07%20MobileNet/">MoblieNet</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">分割任务</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../04%20Segmentation/01%20Introduction/">介绍与基础</a>
</li>

        
            
<li >
    <a href="../../04%20Segmentation/02%20FCN/">FCN</a>
</li>

        
            
<li >
    <a href="../../04%20Segmentation/03%20UNet/">Unet</a>
</li>

        
            
<li >
    <a href="../../04%20Segmentation/04%20DeepLab%E7%B3%BB%E5%88%97/">DeepLab系列</a>
</li>

        
            
<li >
    <a href="../../04%20Segmentation/05%20DANet/">DANet</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">检测</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../05%20Detection/01%20Introduction/">介绍</a>
</li>

        
            
<li >
    <a href="../../05%20Detection/02%20RCNN/">RCNN</a>
</li>

        
            
<li >
    <a href="../../05%20Detection/03%20Fast%20RCNN/">Fast RCNN</a>
</li>

        
            
<li >
    <a href="../../05%20Detection/04%20Faster%20RCNN/">Faster RCNN</a>
</li>

        
            
<li >
    <a href="../../05%20Detection/05%20YOLOv1/">YOLOv1</a>
</li>

        
            
<li >
    <a href="../../05%20Detection/06%20YOLOv2/">YOLOv2</a>
</li>

        
            
<li >
    <a href="../../05%20Detection/07%20YOLOv3/">YOLOv3</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">Transformer</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../06%20Transformer/01%20Debugging/">Transformer</a>
</li>

        
            
<li >
    <a href="../../06%20Transformer/02%20Text%20Rendering/">ViT</a>
</li>

        
            
<li >
    <a href="../../06%20Transformer/03%20SwinTransformer/">SwinTransformer</a>
</li>

        
            
<li >
    <a href="../../06%20Transformer/04%20DETR/">DETR</a>
</li>

        
            
<li >
    <a href="../../06%20Transformer/05%20TransSeg/">TransFormer分割</a>
</li>

        
            
<li >
    <a href="../../06%20Transformer/06%20CLIP/">CLIP</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">AIGC</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../07%20AIGC/01%20Theory/">理论</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">细分方向</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../08%20Subdivision%20direction/Denoise/01%20Paper%20list/">Denoise</a>
</li>

        
            
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">SuperResolution</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../08%20Subdivision%20direction/SuperResolution/01%20SISR%20Paper%20list/">SISR Paper list</a>
</li>

        
            
<li >
    <a href="../../08%20Subdivision%20direction/SuperResolution/02%20VSR%20Paper%20list/">VSR Paper list</a>
</li>

        
    </ul>
  </li>

        
            
<li >
    <a href="../../08%20Subdivision%20direction/zero%20shot/01%20Paper%20list/">zero shot</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">机器学习</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../09%20Mechine%20Learning/01%20Theory/">介绍</a>
</li>

        
    </ul>
  </li>

                    
                        
<li >
    <a href="../../legacy/">历史存档</a>
</li>

                    
                    </ul>
                </li>
            
            
            
                <li >
                    <a href="../../code_repo/">代码仓库</a>
                </li>
            
            
            </ul>

            <!-- Search, Navigation and Repo links -->
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> 搜索
                    </a>
                </li>
                <li >
                    <a rel="next" href="../05%20loss%20functional/">
                        <i class="fa fa-arrow-left"></i> 上一节
                    </a>
                </li>
                <li >
                    <a rel="prev" href="../07%20Selflearning%20XOR/">
                        下一节 <i class="fa fa-arrow-right"></i>
                    </a>
                </li>
                
                <li>
                    <a href="https://github.com/learnAI-CN/learnAI-code">
                        
                            <i class="fa fa-github"></i>
                        
                        GitHub
                    </a>
                </li>
                
                <li>
                    <a href="/img/self.png">
                            <img class="paypal" src="/img/paypal_logo.png" alt="">
                        关于作者
                    </a>
                </li>
            </ul>
        </div>
    </div>
</div>

        <div class="container">
            <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="main active"><a href="#_1">反向传播算法</a></li>
        
            <li><a href="#_2">前向传播梳理</a></li>
        
            <li><a href="#_3">计算损失函数</a></li>
        
            <li><a href="#_4">梯度计算</a></li>
        
            <li><a href="#_5">参数更新</a></li>
        
            <li><a href="#_6">多次迭代</a></li>
        
    
    </ul>
</div></div>
            <div class="col-md-9" role="main">

<h1 id="_1">反向传播算法</h1>
<p><def>反向传播（Backpropagation，BP）</def>算法是一种常用的神经网络训练算法，用于<strong>计算神经网络中每个权重的梯度</strong>，从而进行模型参数的优化。
BP算法的基本思想是通过计算神经网络输出值和真实值之间的误差，然后将误差沿着网络反向传播，计算每个神经元的误差贡献，并根据误差贡献来更新神经网络的权重。</p>
<h2 id="_2">前向传播梳理</h2>
<p>首先我们整体梳理一下前向传播算法。
我们先假设有一个如下的三层神经网络，它包含一个输入层，隐藏层，和输出层。</p>
<p><img alt="MSE" src="../../img/01/06/net.jpg" class="img-center" width="50%" /></p>
<p>首先来重新梳理一下各个参数的含义</p>
<p>
<script type="math/tex">z_{j}^{l}</script>： 表示第l层第j个神经元的输入</p>
<p>
<script type="math/tex">a_{j}^{l}</script>： 表示第l层第j个神经元的输出</p>
<p>
<script type="math/tex">w_{jk}^{l}</script>：表示第l-1层第k个神经元 到 第l层第j个神经元的连接上的权重</p>
<p>
<script type="math/tex">b_{j}^{l}</script>： 表示第l层第j个神经元的偏置</p>
<p>回忆一下第三节单个神经元的输出，</p>
<p>
<script type="math/tex; mode=display">a = \sigma(z) = sigmoid(z) = \frac{1}{1+exp(x * w + b\}}</script>
</p>
<p>可以类似的将隐藏层的三个输出表示为</p>
<p>
<script type="math/tex; mode=display">a_{0}^{2} = \sigma(z_{0}^{2}) = \sigma(w_{00}^{2}*x_{0} + w_{01}^{2}*x_{1} + b_{0}^{2}) </script>
</p>
<p>
<script type="math/tex; mode=display">a_{1}^{2} = \sigma(z_{1}^{2}) = \sigma(w_{10}^{2}*x_{0} + w_{11}^{2}*x_{1} + b_{1}^{2}) </script>
</p>
<p>
<script type="math/tex; mode=display">a_{2}^{2} = \sigma(z_{2}^{2}) = \sigma(w_{20}^{2}*x_{0} + w_{21}^{2}*x_{1} + b_{2}^{2}) </script>
</p>
<p>类似的， 输出层可以表示为</p>
<p>
<script type="math/tex; mode=display">a_{0}^{3} = \sigma(z_{0}^{3}) = \sigma(w_{00}^{3}*a_{0}^{2} + w_{01}^{3}*a_{1}^{2} + w_{02}^{3}*a_{2}^{2} + b_{0}^{3}) </script>
</p>
<p>这里我们要注意了，如果将每个神经元都用公式表示一遍太麻烦了，所以这里我们引入矩阵的概念，将隐藏层直接表示为：</p>
<p>
<script type="math/tex; mode=display">a^{l} = \sigma(z^{l}) = \sigma(W^{l}*a^{l-1} + b^{l}) </script>
</p>
<p>千万别问我为啥能用矩阵表示，你看看列的这么整齐的公式，是不是单独把w提出来位置都不用变就是向量W了。</p>
<h2 id="_3">计算损失函数</h2>
<p>这里我们用上节讲过的MSE Loss来计算损失函数，表达式为：</p>
<p>
<script type="math/tex; mode=display"> C = \frac{1}{2N}\sum_{x}\left \| y - a(x) \right \|^{2} </script>
</p>
<p>其中 <script type="math/tex"> \frac{1}{2N} </script>用于后续优化求导。</p>
<h2 id="_4">梯度计算</h2>
<p>现在在一轮迭代中，我们已经知道了前向过程及当前计算的损失函数。接下来我们希望能够了解这个损失函数怎么指导网络优化权重和偏置。</p>
<p>这里先不做过多的引入，根据大学微积分知识，我们知道在多元函数中，如果只考虑其中某一个自变量对于因变量的影响，就可以使用<def>偏导数（Partial Derivative）</def>来描述这种关系。</p>
<p>首先我们计算一下最后一层代价函数C到输出层神经元的计算。因为C是关于W和b的函数，因此我们在指导W的过程中，可以单独计算W的偏导数，告诉W优化的方向。b同理，接下来先推理一下</p>
<p>C关于W的偏导数推理如下：</p>
<p><img alt="func1" src="../../img/01/06/func1.jpg" class="img-center" width="50%" /></p>
<p>C关于b的偏导数推理如下：</p>
<p><img alt="func2" src="../../img/01/06/func2.jpg" class="img-center" width="50%" /></p>
<p>其中 <script type="math/tex">\odot </script> 符号表示Hadamard 乘积，或者 Schur 乘积。其定义按元素乘积，其中左右两边矩阵必须具有相同的维度。举个例子</p>
<p>
<script type="math/tex; mode=display">
\begin{bmatrix}
1\\ 
2
\end{bmatrix} \odot \begin{bmatrix}
3\\ 
4
\end{bmatrix} = \begin{bmatrix}
1 * 3\\ 
2*4
\end{bmatrix} =  \begin{bmatrix}
3\\ 
8
\end{bmatrix} 
</script>
</p>
<p>由于公式中具有相同的组件，因此为简化函数，我们定义</p>
<p><img alt="func3" src="../../img/01/06/func3.jpg" class="img-center" width="50%" /></p>
<p>如此，损失函数到输出层对权重W和b的求导可以简化为：</p>
<p>
<script type="math/tex; mode=display"> \frac{\partial C(W, b)}{\partial w^{l}} = \delta^l a^{l-1} </script>
</p>
<p>
<script type="math/tex; mode=display"> \frac{\partial C(W, b)}{\partial b^{l}} = \delta^l </script>
</p>
<p>现在我们要计算一下隐藏层的函数，刚才之所以能对输出层求导计算，是因为输出层的w，b是损失函数C的输入。但隐藏层没有这种关系，我们可以将隐藏层w，b作为输出层的函数，进而通过<def>链式法则</def>，从损失函数推导到隐藏层。</p>
<p>假设我们已经求出第l+1层的<script type="math/tex"> \delta^{l+1} </script>, 第l层的计算可以表示为</p>
<p>
<script type="math/tex; mode=display"> \delta^l = \frac{\partial C(W, b)}{\partial z^{l+1}} =  \frac{\partial z^{l+1}}{\partial z^{l}} </script>
</p>
<p>又</p>
<p>
<script type="math/tex; mode=display"> z^{l+1} = W^{l+1}a^l+b^{l+1} =  W^{l+1}\sigma(z^l)+b^{l+1}  </script>
</p>
<p>可求出 </p>
<p><img alt="func4" src="../../img/01/06/func4.jpg" class="img-center" width="30%" /></p>
<p>代人得</p>
<p><img alt="" src="../../img/01/06/func5.jpg" />
<img alt="func5" src="../../img/01/06/func5.jpg" class="img-center" width="30%" /></p>
<p>如此我们便得到的<script type="math/tex"> \delta^l </script>的递推关系，只要求出某一层 <script type="math/tex"> \delta^l </script>，即可方便的求出对该层w和b的梯度。</p>
<h2 id="_5">参数更新</h2>
<p>求出梯度后，我们可以利用梯度下降算法，将梯度乘以一个极小的学习率 <script type="math/tex"> \eta </script>,更新的权重和偏置可以表示为</p>
<p>
<script type="math/tex; mode=display"> w^{l} = w^{l} - \eta \frac{\partial C(W, b)}{\partial w^{l}}  </script>
</p>
<p>
<script type="math/tex; mode=display"> b^{l} = b^{l} - \eta \frac{\partial C(W, b)}{\partial b^{l}}  </script>
</p>
<h2 id="_6">多次迭代</h2>
<p>通过上述的前向传播，计算loss，梯度计算，参数更新等步骤，我们便实现了一次迭代的过程。正如生活会一而再，再而三的打击我们，最终使我们变更优秀一样。
网络也需要经过多次迭代才能最终达到训练的目标。</p>
<p>为了使训练停下来，我们一般会设置固定迭代次数，达到迭代次数后停止。也可以比较相邻两次迭代计算得到的误差。如果误差一直在某个很小的范围，我们认为学习的差不多就这个水平了，然后让网络停止训练。</p>
<p>目前我们已经准备好了使用网络自己学习异或操作的所有知识。接下来一节将用代码实现该内容。</p>

<div id="disqus_thread"></div>
<script>
    (function() {
        var d = document, s = d.createElement('script');

        s.src = '//learnopengl-cn.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>请启用JavaScript以浏览<a href="https://disqus.com/?ref_noscript" rel="nofollow">Disqus评论。</a></noscript></div>
        </div>

        <footer class="col-md-12">
            <hr>
            
            <center>Powered by <a href="http://www.mkdocs.org/">MkDocs</a> and <a href="http://bootswatch.com/yeti/">Yeti</a></center>
        </footer>

        <script src="../../js/jquery-1.10.2.min.js"></script>
        <script src="../../js/bootstrap-3.0.3.min.js"></script>
        <script src="../../js/highlight.pack.js"></script>
        <script>var base_url = '../..';</script>
        <script src="../../js/base.js"></script>
        <script src="../../mathjax/MathJax.js?config=TeX-AMS_HTML"></script>
        <script src="../../search/main.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="modal-header">
                        <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">关闭</span></button>
                        <h4 class="modal-title" id="exampleModalLabel">搜索</h4>
                    </div>
                    <div class="modal-body">
                        <p>
                            请在下面输入你要搜索的文本（仅支持英文）：
                        </p>
                        <form role="form">
                            <div class="form-group">
                                <input type="text" class="form-control" placeholder="搜索..." id="mkdocs-search-query">
                            </div>
                        </form>
                        <div id="mkdocs-search-results"></div>
                    </div>
                    <div class="modal-footer">
                    </div>
                </div>
            </div>
        </div>
    </body>
</html>