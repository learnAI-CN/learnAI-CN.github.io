<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" content="我对DNN知识系统整理的一次尝试">
        
        <link rel="canonical" href="https://learnopengl-cn.github.io/06%20Transformer/01%20Debugging/">
        <link rel="shortcut icon" href="../../img/favicon.ico">

	<title>Transformer - LearnAI-CN</title>

        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome-4.0.3.css" rel="stylesheet">
        <link rel="stylesheet" href="../../css/highlight.css">
        <link href="../../css/base.css" rel="stylesheet">
        <link href="../../css/style.css" rel="stylesheet">
        <link href="../../css/admonition_fix.css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
          ga('create', 'UA-80323542-1', 'auto');
          ga('send', 'pageview');
        </script>
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <!-- Main title -->
            <a class="navbar-brand" href="../..">LearnAI-CN</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            <!-- Main navigation -->
            <ul class="nav navbar-nav">
            
            
                <li >
                    <a href="../..">主页</a>
                </li>
            
            
            
                <li class="dropdown active">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">目录 <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        
<li >
    <a href="../../intro/">简介</a>
</li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">入门</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../01%20Getting%20started/01%20OpenGL/">深度学习介绍</a>
</li>

        
            
<li >
    <a href="../../01%20Getting%20started/02%20Perceptron/">感知机</a>
</li>

        
            
<li >
    <a href="../../01%20Getting%20started/03%20XOR/">异或操作</a>
</li>

        
            
<li >
    <a href="../../01%20Getting%20started/04%20S%20neural/">s型神经元</a>
</li>

        
            
<li >
    <a href="../../01%20Getting%20started/05%20loss%20functional/">损失函数</a>
</li>

        
            
<li >
    <a href="../../01%20Getting%20started/06%20Backpropagation/">反向传播算法</a>
</li>

        
            
<li >
    <a href="../../01%20Getting%20started/07%20Selflearning%20XOR/">自学习异或操作</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">走向深度</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../02%20HardWayToDeep/01%201Channel%20minist/">minist实现</a>
</li>

        
            
<li >
    <a href="../../02%20HardWayToDeep/02%20Gradient%20vanishing/">梯度消失/爆炸</a>
</li>

        
            
<li >
    <a href="../../02%20HardWayToDeep/03%20generalization/">泛化性</a>
</li>

        
            
<li >
    <a href="../../02%20HardWayToDeep/04%20overfitting/">过拟合/欠拟合</a>
</li>

        
            
<li >
    <a href="../../02%20HardWayToDeep/05%20regulization/">正则化</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">实现minist</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../02%20minist/01%20Convolution/">卷积层</a>
</li>

        
            
<li >
    <a href="../../02%20minist/02%20Pooling/">池化层</a>
</li>

        
            
<li >
    <a href="../../02%20minist/03%20Activation/">激活层</a>
</li>

        
            
<li >
    <a href="../../02%20minist/04%20Fully%20conection/">全连接层</a>
</li>

        
            
<li >
    <a href="../../02%20minist/05%20Optimization%20Algorithm/">优化函数</a>
</li>

        
            
<li >
    <a href="../../02%20minist/06%20Normalization/">归一化层</a>
</li>

        
            
<li >
    <a href="../../02%20minist/07%20LeNet/">再次实现minist</a>
</li>

        
            
<li >
    <a href="../../02%20minist/08%20WhyDCNNUseful/">为什么卷积有效</a>
</li>

        
            
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">深入介绍</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../02%20minist/09%20Deep%20introduce/01%20DNN%20feature/">DNN特性</a>
</li>

        
            
<li >
    <a href="../../02%20minist/09%20Deep%20introduce/02%20pooling%20BP/">池化层反向传播</a>
</li>

        
            
<li >
    <a href="../../02%20minist/09%20Deep%20introduce/03%20Receptive%20Field/">感受野</a>
</li>

        
    </ul>
  </li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">经典神经网络</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../03%20classicial%20model/01%20AlexNet/">AlexNet</a>
</li>

        
            
<li >
    <a href="../../03%20classicial%20model/02%20VGG/">VGG</a>
</li>

        
            
<li >
    <a href="../../03%20classicial%20model/03%20GoogleNet/">GoogLeNet</a>
</li>

        
            
<li >
    <a href="../../03%20classicial%20model/04%20ResNet/">ResNet</a>
</li>

        
            
<li >
    <a href="../../03%20classicial%20model/05%20DenseNet/">DenseNet</a>
</li>

        
            
<li >
    <a href="../../03%20classicial%20model/06%20SENet/">SENet</a>
</li>

        
            
<li >
    <a href="../../03%20classicial%20model/07%20MobileNet/">MoblieNet</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">分割任务</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../04%20Segmentation/01%20Introduction/">介绍与基础</a>
</li>

        
            
<li >
    <a href="../../04%20Segmentation/02%20Stencil%20testing/">FCN</a>
</li>

        
            
<li >
    <a href="../../04%20Segmentation/03%20Blending/">Unet</a>
</li>

        
            
<li >
    <a href="../../04%20Segmentation/04%20Face%20culling/">DeepLab系列</a>
</li>

        
            
<li >
    <a href="../../04%20Segmentation/05%20Framebuffers/">DANet</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">检测</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../05%20Detection/01%20Introduction/">介绍</a>
</li>

        
            
<li >
    <a href="../../05%20Detection/02%20RCNN/">RCNN</a>
</li>

        
            
<li >
    <a href="../../05%20Detection/03%20Fast%20RCNN/">Fast RCNN</a>
</li>

        
            
<li >
    <a href="../../05%20Detection/04%20Faster%20RCNN/">Faster RCNN</a>
</li>

        
            
<li >
    <a href="../../05%20Detection/05%20YOLOv1/">YOLOv1</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">Transformer</a>
    <ul class="dropdown-menu">
        
            
<li class="active">
    <a href="./">Transformer</a>
</li>

        
            
<li >
    <a href="../02%20Text%20Rendering/">ViT</a>
</li>

        
            
<li >
    <a href="../03%20SwinTransformer/">SwinTransformer</a>
</li>

        
            
<li >
    <a href="../04%20DETR/">DETR</a>
</li>

        
            
<li >
    <a href="../05%20TransSeg/">TransFormer分割</a>
</li>

        
            
<li >
    <a href="../06%20CLIP/">CLIP</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">AIGC</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../07%20AIGC/01%20Theory/">理论</a>
</li>

        
            
<li >
    <a href="../../07 AIGC/02 minist.md">光照</a>
</li>

        
            
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">IBL</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../07%20AIGC/03%20IBL/01%20Diffuse%20irradiance/">漫反射辐照</a>
</li>

        
            
<li >
    <a href="../../07%20AIGC/03%20IBL/02%20Specular%20IBL/">镜面IBL</a>
</li>

        
    </ul>
  </li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">细分方向</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../08 Subdivision direction/Denoise/01 SISR Paper list.md">Denoise</a>
</li>

        
            
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">SuperResolution</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../08%20Subdivision%20direction/SuperResolution/01%20SISR%20Paper%20list/">SISR Paper list</a>
</li>

        
            
<li >
    <a href="../../08%20Subdivision%20direction/SuperResolution/02%20VSR%20Paper%20list/">VSR Paper list</a>
</li>

        
    </ul>
  </li>

        
            
<li >
    <a href="../../08%20Subdivision%20direction/zero%20shot/01%20Paper%20list/">zero shot</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">机器学习</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../09%20Mechine%20Learning/01%20Theory/">介绍</a>
</li>

        
    </ul>
  </li>

                    
                        
<li >
    <a href="../../legacy/">历史存档</a>
</li>

                    
                    </ul>
                </li>
            
            
            
                <li >
                    <a href="../../code_repo/">代码仓库</a>
                </li>
            
            
            </ul>

            <!-- Search, Navigation and Repo links -->
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> 搜索
                    </a>
                </li>
                <li >
                    <a rel="next" href="../../05%20Detection/05%20YOLOv1/">
                        <i class="fa fa-arrow-left"></i> 上一节
                    </a>
                </li>
                <li >
                    <a rel="prev" href="../02%20Text%20Rendering/">
                        下一节 <i class="fa fa-arrow-right"></i>
                    </a>
                </li>
                
                <li>
                    <a href="https://github.com/LearnOpenGL-CN/LearnOpenGL-CN">
                        
                            <i class="fa fa-github"></i>
                        
                        GitHub
                    </a>
                </li>
                
                <li>
                    <a href="https://www.paypal.me/learnopengl/">
                            <img class="paypal" src="/img/paypal_logo.png" alt="">
                        支持原作者
                    </a>
                </li>
            </ul>
        </div>
    </div>
</div>

        <div class="container">
            <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="main active"><a href="#transformer">Transformer</a></li>
        
            <li><a href="#_1">整体结构</a></li>
        
            <li><a href="#self-attention">Self-Attention</a></li>
        
            <li><a href="#positional-encoding">位置编码(Positional Encoding)</a></li>
        
            <li><a href="#encoder">Encoder其他组成部分</a></li>
        
            <li><a href="#decoder">Decoder</a></li>
        
            <li><a href="#_6">回到论文</a></li>
        
            <li><a href="#_7">总结</a></li>
        
            <li><a href="#_8">参考</a></li>
        
    
    </ul>
</div></div>
            <div class="col-md-9" role="main">

<h1 id="transformer">Transformer</h1>
<p>Transformer是目前深度学习领域最前沿的研究方向，虽说他从之前网络的Attention机制演化而来，但是却又和传统CNN，RNN之类完全不同。</p>
<p>Transformer在论文《Attention Is All You Need》中首次被提出，用来实现翻译任务。他通过使用注意力机制来处理序列中的长距离依赖关系，从而提高模型的表达能力和泛化能力。
此后拓展到图形处理领域，并在如今用于多模态学习，AIGC等。</p>
<p>本文将首先介绍其实现过程，然后再介绍其原理和有效性。</p>
<h2 id="_1">整体结构</h2>
<p>我们可以先从整体来看Transformer的结构。现在我们来想象一个翻译的场景，</p>
<p><img alt="" src="../../img/06/01/img1.png" /></p>
<p>我们可以把他当作一个黑盒子，就像之前的CNN一样，给他一个输出，就能得到对应的输出。</p>
<p>然后我们将网络进一步展开，</p>
<p><img alt="" src="../../img/06/01/img2.png" /></p>
<p>Transformer包含Encoders和Decoders两个部分，进一步来说，这两部分又分别包含若干个小的编解码结构。</p>
<p><img alt="" src="../../img/06/01/img3.png" /></p>
<p>输入通过多个Encoder逐个处理，每个Encoder的输出会作为下一个Encoder的输入，最后一个Encoder的输出需要作为每一个Decoder的输入。</p>
<p>现在我们再来看看每一个Encoder和Decoder具体又包含了哪些东西</p>
<p><img alt="" src="../../img/06/01/img4.png" /></p>
<p>每一层的Encoder都是由一个Self-Attention层和一个前馈网络(Feed Forward Neural Network)组成。</p>
<p>而Decoder则是由Self-Attention、Encoder-decoder Attention和Feed Forward三个部分组成。多的Encoder-decoder Attention
主要是为了在解码时会考虑最后一层Encoder所有时刻的输出。</p>
<h2 id="self-attention">Self-Attention</h2>
<h3 id="_2">引入</h3>
<p>首先我们考虑要翻译”The animal didn’t cross the street because it was too tired”(这个动物无法穿越马路，因为它太累了)。
现在想知道句中的&rdquo;it&rdquo;指代的是&rdquo;animal&rdquo;还是&rdquo;street&rdquo;，我们可以<strong>根据后文</strong>中的&rdquo;tired&rdquo;来判断it指代的是<strong>前文</strong>中的animal。
因为动物才可能累。这段理解对我们来说是很简单的，但是对之前的工作如RNN，LSTM等都无法实现，因为他们没有办法同时考虑前文和后文中的内容。</p>
<p>而self Attention的目的就在于，每次编码某个单词的时候，都要考虑句子中所有的其他单词。接下来我们看他是具体怎么做到的。</p>
<h3 id="_3">单个时间实现</h3>
<p>如下图所示，我们先将输入的词进行embedding，得到词向量表示x1，x2。接着使用矩阵<script type="math/tex"> W^Q </script>, <script type="math/tex"> W^K </script>, <script type="math/tex"> W^V </script>对输入进行变换，
得到向量<script type="math/tex"> q_1，k_1， v_1 </script> 和 <script type="math/tex"> q_2，k_2， v_2 </script>，其中 </p>
<p>
<script type="math/tex; mode=display"> q_t = x_tW^Q </script>
</p>
<p><img alt="" src="../../img/06/01/img5.png" /></p>
<p>k,v类似。为了后续Q，K可以内积，我们需要<script type="math/tex"> W^Q </script>和<script type="math/tex"> W^K </script>维度相同。<script type="math/tex"> W^V </script>则没有此要求。</p>
<p>得到 <script type="math/tex"> Q_t，K_t， V_t </script>之后，我们可以计算self Attention了。即用Q去查询每一个V，我们以第一时刻为例，首先计算<script type="math/tex">q_1</script>和
<script type="math/tex">k_1</script>, <script type="math/tex">k_2</script>之间的内积，这里的Score等于112和96是种假设值。</p>
<p><img alt="" src="../../img/06/01/img6.png" /></p>
<p>然后将Score除以8之后再进行softmax计算，把得分变为概率。至于为什么是除以8，是因为8是作者使用的<script type="math/tex"> \sqrt{d_k} </script>计算而来的，<script type="math/tex"> d_k </script>又是k向量的维度。
那为啥要除以k向量维度的平方根呢？是因为作者认为这样计算梯度时会更加稳定。</p>
<p><img alt="" src="../../img/06/01/img7.png" /></p>
<p>最后用softmax得到的概率对所有时刻的V求加权平均，得到输出Z，作者认为Z根据Self-Attention的概率综合考虑了所有时刻的输入信息，计算过程如下图所示。</p>
<p><img alt="" src="../../img/06/01/img8.png" /></p>
<p>以上演示了输入为Thinking这一个时间步的计算过程。计算其他时刻是一样的，我们将用矩阵形式描述该过程。</p>
<h3 id="_4">矩阵实现</h3>
<p>对不同的时间步，我们通过将输入拼接为一个矩阵进行并行训练。如下图所示，</p>
<p><img alt="" src="../../img/06/01/img9.png" /></p>
<p>此处X为两维分别代表Thinking 和 Mechines 得到的两个向量输入。同时经过矩阵运算得到输出QKV，当然如果输入的词向量足够多，也可以是其他维度。</p>
<p>然后和之前完全相同，利用Q和K计算得到Score，除以<script type="math/tex"> \sqrt{d_k} </script>后送入Softmax，最后加权平均得到输出。
该过程表示如下：</p>
<p><img alt="" src="../../img/06/01/img10.png" /></p>
<h3 id="multi-head-attention">Multi-Head Attention</h3>
<p>通过矩阵实现，我们将输入经过<strong>一组</strong>QKV的计算过程表示了出来，但是接下来我们希望定义多组QKV，让他们分别关注不同的上下文，提升网络的表征能力。
每组的计算过程相同，变换矩阵表示为<script type="math/tex"> (W_i^Q, W_i^K, W_i^V) </script>, 其中i=(1,2,..,n)。这里n表示有几组Multi-Head。</p>
<p><img alt="" src="../../img/06/01/img11.png" /></p>
<p>图中展示了一组输入经过n=8的multi Head得到的8组输出<script type="math/tex"> (Z_0, Z_1, .. , Z_7) </script>。</p>
<p>接下来我们将这8组输出通过维度拼接转为一个矩阵，然后利用线性变换（乘矩阵 <script type="math/tex">W_0</script>）对它进行压缩来减少维度，该过程具体如下</p>
<p><img alt="" src="../../img/06/01/img12.png" /></p>
<p>综上，self Attention过程介绍完了，我们用一个完整的图示表示如下</p>
<p><img alt="" src="../../img/06/01/img13.png" /></p>
<p>这里R在前面没有提起，因为之前X表示第一层encoder词向量的输入，之后用R表示其他层encoder的输入。</p>
<h3 id="_5">有效性</h3>
<p>现在我们回头看来引言中的例子，看一看self Attention的作用。下图为第六层Encoder中不同Head Attention的可视化图。</p>
<p><img alt="" src="../../img/06/01/img14.png" />
<img alt="" src="../../img/06/01/img15.png" /></p>
<p>可以看到不同的Head关注的重点是不同的，其中第一个图中it确实更为关注句中的&rdquo;animal&rdquo;。</p>
<h2 id="positional-encoding">位置编码(Positional Encoding)</h2>
<p>正所谓尺有所短，寸有所长。RNN和LSTM这些结构虽然不能全面的看待句子中所有位置单词的信息，但他们对应的有点是本身包含了词向量的语义信息。
但self attention虽然可以对句子中任意位置的单词进行编码计算权重，但缺少位置信息。所以我们需要对self Attention的输入进行修改，嵌入位置信息。</p>
<p>通过引入一个新的位置向量，该向量与时间t相关。我们把Embedding和位置编码向量加起来作为模型的输入。图示如下，</p>
<p><img alt="" src="../../img/06/01/posEmbedding.jpg" /></p>
<p>这样的话如果两个词在不同的位置出现了，虽然它们的Embedding是相同的，但是由于位置编码不同，最终得到的向量也是不同的。</p>
<p>位置编码有很多方法，此处不再细讲。</p>
<h2 id="encoder">Encoder其他组成部分</h2>
<p>之前我们提到Encoder可以分为Self Attention 和 Feed Forward两大部分，但这只是粗略的划分，其内部具体结构如下图所示</p>
<p><img alt="" src="../../img/06/01/enc.png" /></p>
<p>如图展示了其细节组成，可具体分为以下步骤：
- 1、输入和位置编码一同送入Encoder
- 2、x1，x2经过self Attention的到z1，z2。然后和短接的输入x1，x2相加，送入layerNorm层。
- 3、步骤2的输出再经过前馈层（feed forward）层，该层实际为全连接层。
- 4、前馈层的输出同样与短接的z1，z2相加，再利用layerNorm归一化。
- 5、最终的输出作为下一个Encoder的输入。</p>
<h2 id="decoder">Decoder</h2>
<p>现在我们合起来看看Encoder和Decoder的结构</p>
<p><img alt="" src="../../img/06/01/enc-dec.png" /></p>
<p>Decoder和Encoder是类似的，区别在于它多了一个Encoder-Decoder Attention层。他的Query来自下一层，而Key和Value则来自最后一层Encoder的所有时刻的输出。</p>
<p>可能你想问Encoder-Decoder Attention结构是什么样的，答案是和self Attention一模一样。他们只是输入有区别。</p>
<h2 id="_6">回到论文</h2>
<p>现在我们来引入论文中，你可能会见过无数次的图。</p>
<p><img alt="" src="../../img/06/01/transformer.png" /></p>
<p>现在你应该很容易理解每个部分的作用及结构了，但还有三点需要提一下。</p>
<p>1、最后一个Encoder的输出有两个箭头，送入Decoder的Encoder-Decoder Attention结构，他们就是刚说的key和value。</p>
<p>2、Decoder中的Masked Multi-Head Attention中Mask的作用是屏蔽掉未来的信息，从而避免在解码器中使用未来的信息来影响当前的预测结果。
因为在训练过程中，解码器是逐步生成输出序列的，每个时间步只能使用当前和之前的信息。如果使用未来的信息，就会导致模型出现信息泄漏的问题，影响模型的预测结果。</p>
<p>3、Decoder最终的输出经过一个线性层和一个softmax得到最终的预测输出。</p>
<h2 id="_7">总结</h2>
<p>至此，Transformer的完整结构就讲完了，接下来我们会通过几篇CV领域应用Transformer的论文介绍其应用。
可以简单提一嘴的是这些论文一般只采用Encoder即可。</p>
<h2 id="_8">参考</h2>
<p>http://jalammar.github.io/illustrated-transformer/</p>

<div id="disqus_thread"></div>
<script>
    (function() {
        var d = document, s = d.createElement('script');

        s.src = '//learnopengl-cn.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>请启用JavaScript以浏览<a href="https://disqus.com/?ref_noscript" rel="nofollow">Disqus评论。</a></noscript></div>
        </div>

        <footer class="col-md-12">
            <hr>
            
            <center>Powered by <a href="http://www.mkdocs.org/">MkDocs</a> and <a href="http://bootswatch.com/yeti/">Yeti</a></center>
        </footer>

        <script src="../../js/jquery-1.10.2.min.js"></script>
        <script src="../../js/bootstrap-3.0.3.min.js"></script>
        <script src="../../js/highlight.pack.js"></script>
        <script>var base_url = '../..';</script>
        <script src="../../js/base.js"></script>
        <script src="../../mathjax/MathJax.js?config=TeX-AMS_HTML"></script>
        <script src="../../search/main.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="modal-header">
                        <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">关闭</span></button>
                        <h4 class="modal-title" id="exampleModalLabel">搜索</h4>
                    </div>
                    <div class="modal-body">
                        <p>
                            请在下面输入你要搜索的文本（仅支持英文）：
                        </p>
                        <form role="form">
                            <div class="form-group">
                                <input type="text" class="form-control" placeholder="搜索..." id="mkdocs-search-query">
                            </div>
                        </form>
                        <div id="mkdocs-search-results"></div>
                    </div>
                    <div class="modal-footer">
                    </div>
                </div>
            </div>
        </div>
    </body>
</html>