<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" content="我对DNN知识系统整理的一次尝试">
        
        <link rel="canonical" href="https://learnai-cn.github.io/04%20Segmentation/05%20DANet/">
        <link rel="shortcut icon" href="../../img/favicon.ico">

	<title>DANet - LearnAI-CN</title>

        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome-4.0.3.css" rel="stylesheet">
        <link rel="stylesheet" href="../../css/highlight.css">
        <link href="../../css/base.css" rel="stylesheet">
        <link href="../../css/style.css" rel="stylesheet">
        <link href="../../css/admonition_fix.css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
          ga('create', 'UA-80323542-1', 'auto');
          ga('send', 'pageview');
        </script>
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <!-- Main title -->
            <a class="navbar-brand" href="../..">LearnAI-CN</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            <!-- Main navigation -->
            <ul class="nav navbar-nav">
            
            
                <li >
                    <a href="../..">主页</a>
                </li>
            
            
            
                <li class="dropdown active">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">目录 <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        
<li >
    <a href="../../intro/">简介</a>
</li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">入门</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../01%20Getting%20started/01%20Introduction/">深度学习介绍</a>
</li>

        
            
<li >
    <a href="../../01%20Getting%20started/02%20Perceptron/">感知机</a>
</li>

        
            
<li >
    <a href="../../01%20Getting%20started/03%20XOR/">异或操作</a>
</li>

        
            
<li >
    <a href="../../01%20Getting%20started/04%20S%20neural/">s型神经元</a>
</li>

        
            
<li >
    <a href="../../01%20Getting%20started/05%20loss%20functional/">损失函数</a>
</li>

        
            
<li >
    <a href="../../01%20Getting%20started/06%20Backpropagation/">反向传播算法</a>
</li>

        
            
<li >
    <a href="../../01%20Getting%20started/07%20Selflearning%20XOR/">自学习异或操作</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">走向深度</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../02%20HardWayToDeep/01%201Channel%20minist/">minist实现</a>
</li>

        
            
<li >
    <a href="../../02%20HardWayToDeep/02%20Gradient%20vanishing/">梯度消失/爆炸</a>
</li>

        
            
<li >
    <a href="../../02%20HardWayToDeep/03%20generalization/">泛化性</a>
</li>

        
            
<li >
    <a href="../../02%20HardWayToDeep/04%20overfitting/">过拟合/欠拟合</a>
</li>

        
            
<li >
    <a href="../../02%20HardWayToDeep/05%20regulization/">正则化</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">实现minist</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../02%20minist/00%20Introduction/">介绍</a>
</li>

        
            
<li >
    <a href="../../02%20minist/01%20Convolution/">卷积层</a>
</li>

        
            
<li >
    <a href="../../02%20minist/02%20Pooling/">池化层</a>
</li>

        
            
<li >
    <a href="../../02%20minist/03%20Activation/">激活层</a>
</li>

        
            
<li >
    <a href="../../02%20minist/04%20Fully%20conection/">全连接层</a>
</li>

        
            
<li >
    <a href="../../02%20minist/05%20Optimization%20Algorithm/">优化函数</a>
</li>

        
            
<li >
    <a href="../../02%20minist/06%20Normalization/">归一化层</a>
</li>

        
            
<li >
    <a href="../../02%20minist/07%20LeNet/">LeNet实现minist</a>
</li>

        
            
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">深入介绍</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../02%20minist/09%20Deep%20introduce/01%20DNN%20feature/">DNN特性</a>
</li>

        
            
<li >
    <a href="../../02%20minist/09%20Deep%20introduce/02%20pooling%20BP/">池化层反向传播</a>
</li>

        
            
<li >
    <a href="../../02%20minist/09%20Deep%20introduce/03%20Receptive%20Field/">感受野</a>
</li>

        
            
<li >
    <a href="../../02%20minist/09%20Deep%20introduce/04%20WhyDCNNUseful/">为什么卷积有效</a>
</li>

        
    </ul>
  </li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">经典神经网络</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../03%20classicial%20model/01%20AlexNet/">AlexNet</a>
</li>

        
            
<li >
    <a href="../../03%20classicial%20model/02%20VGG/">VGG</a>
</li>

        
            
<li >
    <a href="../../03%20classicial%20model/03%20GoogleNet/">GoogLeNet</a>
</li>

        
            
<li >
    <a href="../../03%20classicial%20model/04%20ResNet/">ResNet</a>
</li>

        
            
<li >
    <a href="../../03%20classicial%20model/05%20DenseNet/">DenseNet</a>
</li>

        
            
<li >
    <a href="../../03%20classicial%20model/06%20SENet/">SENet</a>
</li>

        
            
<li >
    <a href="../../03%20classicial%20model/07%20MobileNet/">MoblieNet系列</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">分割任务</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../01%20Introduction/">介绍与基础</a>
</li>

        
            
<li >
    <a href="../02%20FCN/">FCN</a>
</li>

        
            
<li >
    <a href="../03%20UNet/">Unet</a>
</li>

        
            
<li >
    <a href="../04%20DeepLab%E7%B3%BB%E5%88%97/">DeepLab系列</a>
</li>

        
            
<li class="active">
    <a href="./">DANet</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">检测</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../05%20Detection/01%20Introduction/">介绍</a>
</li>

        
            
<li >
    <a href="../../05%20Detection/02%20RCNN/">RCNN</a>
</li>

        
            
<li >
    <a href="../../05%20Detection/03%20Fast%20RCNN/">Fast RCNN</a>
</li>

        
            
<li >
    <a href="../../05%20Detection/04%20Faster%20RCNN/">Faster RCNN</a>
</li>

        
            
<li >
    <a href="../../05%20Detection/05%20YOLOv1/">YOLOv1</a>
</li>

        
            
<li >
    <a href="../../05%20Detection/06%20YOLOv2/">YOLOv2</a>
</li>

        
            
<li >
    <a href="../../05%20Detection/07%20YOLOv3/">YOLOv3</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">Transformer</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../06%20Transformer/01%20Debugging/">Transformer</a>
</li>

        
            
<li >
    <a href="../../06%20Transformer/02%20ViT/">ViT</a>
</li>

        
            
<li >
    <a href="../../06%20Transformer/03%20SwinTransformer/">SwinTransformer</a>
</li>

        
            
<li >
    <a href="../../06%20Transformer/04%20DETR/">DETR</a>
</li>

        
            
<li >
    <a href="../../06%20Transformer/05%20TransSeg/">TransFormer分割</a>
</li>

        
            
<li >
    <a href="../../06%20Transformer/06%20CLIP/">CLIP</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">AIGC</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../07%20AIGC/01%20Theory/">理论</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">细分方向</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../08%20Subdivision%20direction/Denoise/01%20Paper%20list/">Denoise</a>
</li>

        
            
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">SuperResolution</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../08%20Subdivision%20direction/SuperResolution/01%20SISR%20Paper%20list/">SISR Paper list</a>
</li>

        
            
<li >
    <a href="../../08%20Subdivision%20direction/SuperResolution/02%20VSR%20Paper%20list/">VSR Paper list</a>
</li>

        
    </ul>
  </li>

        
            
<li >
    <a href="../../08%20Subdivision%20direction/zero%20shot/01%20Paper%20list/">zero shot</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">机器学习</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../09%20Mechine%20Learning/01%20Theory/">介绍</a>
</li>

        
    </ul>
  </li>

                    
                        
<li >
    <a href="../../legacy/">历史存档</a>
</li>

                    
                    </ul>
                </li>
            
            
            
                <li >
                    <a href="../../code_repo/">代码仓库</a>
                </li>
            
            
            </ul>

            <!-- Search, Navigation and Repo links -->
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> 搜索
                    </a>
                </li>
                <li >
                    <a rel="next" href="../04%20DeepLab%E7%B3%BB%E5%88%97/">
                        <i class="fa fa-arrow-left"></i> 上一节
                    </a>
                </li>
                <li >
                    <a rel="prev" href="../../05%20Detection/01%20Introduction/">
                        下一节 <i class="fa fa-arrow-right"></i>
                    </a>
                </li>
                
                <li>
                    <a href="https://github.com/learnAI-CN/learnAI-code">
                        
                            <i class="fa fa-github"></i>
                        
                        GitHub
                    </a>
                </li>
                
                <li>
                    <a href="/img/self.png">
                            <img class="paypal" src="/img/paypal_logo.png" alt="">
                        关于作者
                    </a>
                </li>
            </ul>
        </div>
    </div>
</div>

        <div class="container">
            <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="main active"><a href="#danet">DANet</a></li>
        
            <li><a href="#_1">创新点</a></li>
        
            <li><a href="#_2">注意力</a></li>
        
            <li><a href="#_3">整体结构</a></li>
        
            <li><a href="#position-attention-module">Position Attention Module</a></li>
        
            <li><a href="#channel-attention-module">Channel Attention Module</a></li>
        
            <li><a href="#_4">优缺点</a></li>
        
            <li><a href="#_5">讨论</a></li>
        
            <li><a href="#_6">参考</a></li>
        
    
    </ul>
</div></div>
            <div class="col-md-9" role="main">

<h1 id="danet">DANet</h1>
<p>DANet是将<def>注意力机制</def>与分割结合的典型作品，我希望能通过该论文的讲解给您引入对注意力的理解。</p>
<h2 id="_1">创新点</h2>
<p>引入空间注意力和通道注意力</p>
<h2 id="_2">注意力</h2>
<p>之前我们在学习SENet的时候了解过<def>通道注意力</def>机制，其目的在于使网络对通道级特征进行重标定。
本文意在通过<def>空间注意力</def>和<def>通道注意力</def>，对分割网络的空间特征和通道特征都进行重标定，优化网络特征表示。</p>
<h2 id="_3">整体结构</h2>
<p>网络的整体结构示意图如下所示</p>
<p><img alt="" src="../../img/04/05/DANet.png" /></p>
<p>1、网络首先使用Dilated ResNet作为主干网络，用于特征提取。和DeepLab V2一样，最终获得1/8将采样尺寸的特征图。</p>
<p>2、将特征图同时送入空间和通道注意力机制，得到加权后的特征图。</p>
<p>3、通过元素相加融合两个特征图，进而通过卷积得到最终的预测输出。</p>
<h2 id="position-attention-module">Position Attention Module</h2>
<p>位置注意力模块如下图所示，目的在于获得空间加权的特征输出E。</p>
<p><img alt="" src="../../img/04/05/PAM.jpg" /></p>
<p>该模块具体流程如下:</p>
<p>1、特征图A[C, H, W]首先分别通过3个卷积层得到3个特征图B,C,D。输出尺寸为[C, H, W]。</p>
<p>2、将B，C先reshape为[C, H×W],再将B进行转置，获得输出维度[H×W, C]。将两者相乘，再通过softmax获得空间特征映射S（Spatial Attention map）,尺寸为[H×W, H×W]。</p>
<p>3、将D reshape为[C, H×W]，再和S的转置进行矩阵乘，获得[C, H×W]的输出，再乘以尺度系数α，最后reshape 为 [C, H, W]。
此处α 初始化为0，并在训练过程中逐渐增加。</p>
<p>4、将步骤3的输出与输入A相加，得到最终输出E。</p>
<p>该模块代码实现如下</p>
<pre><code class="language-python">class PAM_Module(Module):
    &quot;&quot;&quot; Position attention module&quot;&quot;&quot;
    def __init__(self, in_dim):
        super(PAM_Module, self).__init__()
        self.chanel_in = in_dim

        self.query_conv = Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)
        self.key_conv = Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)
        self.value_conv = Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)
        self.gamma = Parameter(torch.zeros(1))

        self.softmax = Softmax(dim=-1)
    def forward(self, x):
        &quot;&quot;&quot;
            inputs :
                x : input feature maps( B X C X H X W)
            returns :
                out : attention value + input feature
                attention: B X (HxW) X (HxW)
        &quot;&quot;&quot;
        m_batchsize, C, height, width = x.size()
        proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1)
        proj_key = self.key_conv(x).view(m_batchsize, -1, width*height)
        energy = torch.bmm(proj_query, proj_key)
        attention = self.softmax(energy)
        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height)

        out = torch.bmm(proj_value, attention.permute(0, 2, 1))
        out = out.view(m_batchsize, C, height, width)

        out = self.gamma*out + x
        return out

</code></pre>
<h2 id="channel-attention-module">Channel Attention Module</h2>
<p>通道注意力模块如下图所示，目的在于对通道维特征进行筛选加权。</p>
<p><img alt="" src="../../img/04/05/CAM.jpg" /></p>
<p>该过程类似与PAM，区别在于获得通道注意力特征图X，尺寸为[C, C]。</p>
<p>接着把X的转置[C, C]与reshape的A[C, N]做矩阵乘法，再乘以尺度系数β，再reshape为原来形状，最后与A相加得到最后的输出E。</p>
<p>CAM的代码实现如下</p>
<pre><code class="language-python">class CAM_Module(Module):
    &quot;&quot;&quot; Channel attention module&quot;&quot;&quot;
    def __init__(self, in_dim):
        super(CAM_Module, self).__init__()
        self.chanel_in = in_dim


        self.gamma = Parameter(torch.zeros(1))
        self.softmax  = Softmax(dim=-1)
    def forward(self,x):
        &quot;&quot;&quot;
            inputs :
                x : input feature maps( B X C X H X W)
            returns :
                out : attention value + input feature
                attention: B X C X C
        &quot;&quot;&quot;
        m_batchsize, C, height, width = x.size()
        proj_query = x.view(m_batchsize, C, -1)
        proj_key = x.view(m_batchsize, C, -1).permute(0, 2, 1)
        energy = torch.bmm(proj_query, proj_key)
        energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy)-energy
        attention = self.softmax(energy_new)
        proj_value = x.view(m_batchsize, C, -1)

        out = torch.bmm(attention, proj_value)
        out = out.view(m_batchsize, C, height, width)

        out = self.gamma*out + x
        return out


</code></pre>
<h2 id="_4">优缺点</h2>
<p>DANet中的注意力机制设计简单明了，可以适应不同尺寸输入。 可以根据输入的特征图动态地调整权重，从而使模型更加关注有用的信息，忽略无用的信息。</p>
<p>但DANet计算量过大，参数过多，所以难以向SE Block一样用在网络的多个位置，仅可使用在特征提取的高维来减少计算量。
此外还可以在第一步获得特征图B，C，D的时候通过卷积降低通道维度，减少计算量。上文中PAM的代码就是这样实现的。</p>
<h2 id="_5">讨论</h2>
<p>等后续学习Transformer之后可以感觉本文提出的注意力机制和Transformer有一定相似度。你可以把三个特征图B，C，D作为Q，K，V。
同样是Q和K的转置相乘，然后通过softmax，最后和V相乘。</p>
<p>如果使用公式表示就更明显了，DANet在获得空间权重映射的操作可以表示为：</p>
<p>
<script type="math/tex; mode=display"> Att_p = softmax(CB^T)^TD </script>
</p>
<p>而Transformer的注意力机制可以表示为：</p>
<p>
<script type="math/tex; mode=display"> Att = softmax(\frac{QK^T}{\sqrt{d_k}})V  </script>
</p>
<h2 id="_6">参考</h2>
<p>https://github.com/junfu1115/DANet</p>

<div id="disqus_thread"></div>
<script>
    (function() {
        var d = document, s = d.createElement('script');

        s.src = '//learnopengl-cn.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>请启用JavaScript以浏览<a href="https://disqus.com/?ref_noscript" rel="nofollow">Disqus评论。</a></noscript></div>
        </div>

        <footer class="col-md-12">
            <hr>
            
            <center>Powered by <a href="http://www.mkdocs.org/">MkDocs</a> and <a href="http://bootswatch.com/yeti/">Yeti</a></center>
        </footer>

        <script src="../../js/jquery-1.10.2.min.js"></script>
        <script src="../../js/bootstrap-3.0.3.min.js"></script>
        <script src="../../js/highlight.pack.js"></script>
        <script>var base_url = '../..';</script>
        <script src="../../js/base.js"></script>
        <script src="../../mathjax/MathJax.js?config=TeX-AMS_HTML"></script>
        <script src="../../search/main.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="modal-header">
                        <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">关闭</span></button>
                        <h4 class="modal-title" id="exampleModalLabel">搜索</h4>
                    </div>
                    <div class="modal-body">
                        <p>
                            请在下面输入你要搜索的文本（仅支持英文）：
                        </p>
                        <form role="form">
                            <div class="form-group">
                                <input type="text" class="form-control" placeholder="搜索..." id="mkdocs-search-query">
                            </div>
                        </form>
                        <div id="mkdocs-search-results"></div>
                    </div>
                    <div class="modal-footer">
                    </div>
                </div>
            </div>
        </div>
    </body>
</html>